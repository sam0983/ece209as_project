# Abstract

AIoT (Artiﬁcial Intelligence of Things) is a relatively new term that has recently become a hot topic that combines two of the hottest acronyms, AI (Artiﬁcial Intelligence) andIoT (Internet of Things). IoT consists of interconnected things with built-in sensors and has the potential to generate or collect a vast amount of data. With that comes a lot of collected or real-time data, an intelligent and eﬃcient data processing is essential to make eﬀective use of the information generated from these data. The data can be analyzed and utilized with AI for problem-solving or decision-making. However, there are many challenges while deploying AIoT in practice. For instance, machine learning is one of the key technologies to be utilized in AIoT systems. There are two common issues in the practice of AIoT. The first one is that the edge device has relatively computational resources, so it will be difficult for edge devices to perform real-time inference with a very deep or complex deep learning model. As a result, edge devices that perform computations on real-time data are usually limited to using lightweight models with less complexity, such as tiny-YOLO or SqueezeNet, which is computationally efficent but not as accurate compared to deep models. The second issue is closely related to the second issue, being that to incorporate deeper models in AIoT systems, the actual computations are implemented in the cloud devices, whereas the edge device merely serves as a data collecting device, sending all data to the cloud for computation. While this may yield better results in terms of accuracy, there is the problem of latency when we send all data to the cloud continously. Also, it may be expensive to transmit so much data when we are using a paid cloud platform such as AWS, which charges you according to the data transmission and computation. The first and second issue is usually a trade-off, having to choose between accuracy and cost. The objective of this research project is to tackle the problem mentioned above. We attempt to use DNN partitioning techniques to slice the computations of a very deep model such as ResNet-152 or VGG-19 into two parts. The edge device computes the first few layers of the model while the cloud computes the rest. This method reduces the workload of the edge device, and it also won't send the entire image to the cloud, which could reduce the data transmission latency. In our research, we put a twist to DNN partitioning, while traditional DNN partitioning methods still continously send data to the cloud, we add a classifier at the end of the first segment (edge device) of the model, we then calculate the uncertainty of the output using Monte Carlo Dropout. If the calculated uncertainty is within a certain threshold, it is deemed that the edge device segment of the model is confident, so it will be unneccessary to send the data to the cloud. Likewise, if the uncertainty passes the threshold, the output before the classification layer will be transmitted to the cloud for further computation by the rest of the deep model to obtain possibly better results. Experiments are implemented to determine the effectiveness of this method.

# Team

* Liang-Chien Liu

# Required Submissions

* [Proposal](proposal)
* [Midterm Checkpoint Presentation Slides](http://)
* [Final Presentation Slides](http://)
* [Final Report](report)
